\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{graphicx}
\geometry{margin=1in}

\title{Flash-SD-KDE}
\author{}
\date{}

\begin{document}
\maketitle

\section{Background}

Score-debiased kernel density estimation (SD-KDE) refines a standard kernel
density estimator by moving the data slightly in the direction of the score
function (the gradient of the log-density).  Given samples $x_i$ and a
bandwidth $h$, a Gaussian KDE is
\[
  \hat p(x)
  =
  \frac{1}{n h}
  \sum_{i=1}^n
  \varphi\!\left(\frac{x - x_i}{h}\right),
\]
and SD-KDE forms debiased samples
$x_i^{\mathrm{SD}} = x_i + \tfrac{h^2}{2}\,\hat s(x_i)$ where
$\hat s$ is an estimate of the score.  In empirical SD-KDE, this score
is itself computed from the KDE, so no model of the underlying density
is required.  Writing $z_i(x) = (x - x_i)/h$ and
\[
  \hat p(x)
  =
  \frac{1}{n h}
  \sum_{i=1}^n
  \varphi\bigl(z_i(x)\bigr),
\]
we estimate the score as
\[
  \hat s(x)
  =
  \frac{\partial_x \hat p(x)}{\hat p(x)}
  =
  \frac{\sum_{i=1}^n \bigl[-z_i(x)\,\varphi(z_i(x))\bigr]}
       {h^2 \sum_{i=1}^n \varphi(z_i(x))}.
\]

\section{Task}

The goal of Flash-SD-KDE is to implement a fast, GPU-accelerated empirical
SD-KDE on an NVIDIA RTX~A6000 using Triton.  We:
\begin{itemize}
  \item implement a Gaussian KDE and KDE-based score estimator in Triton,
  \item use these to construct an empirical SD-KDE on the GPU, and
  \item benchmark against CPU Silverman KDE and scikit-learn's KDE over
        a range of sample sizes $(n_{\text{train}}, n_{\text{test}})$.
\end{itemize}

\section{Arithmetic intensity of KDE}

Arithmetic intensity is defined as
\[
  I \;=\; \frac{\text{floating-point operations}}{\text{bytes moved to/from DRAM}}.
\]
For a Gaussian KDE evaluated at $m$ query points using $n$ training points,
we perform $O(mn)$ pairwise interactions.  For the density-only KDE, each
pair involves differences, scaling, squaring and an exponential; counting
subtractions, multiplications, an $\exp$ and a few additions gives a
per-pair cost of roughly $20$--$30$ flops.  Empirical SD-KDE also requires
the score, so we accumulate both $\varphi(z_i)$ and $-z_i\varphi(z_i)$ and
later form their ratio, adding extra flops.  Overall, a conservative estimate
for the empirical SD-KDE pipeline is $c \approx 50$ flops per interaction.

Thus the total flops scale as
\[
  \text{FLOPs} \approx c\, m n.
\]
With a properly tiled GPU implementation, each training point and each query
is loaded from global memory only once and then reused from registers or
shared memory.  The number of bytes moved is therefore
\[
  \text{Bytes} \approx 4\,(m + n)
\]
for single-precision floats, up to lower-order terms for writing the output.
The arithmetic intensity is then
\[
  I
  \approx
  \frac{c\,m n}{4\,(m+n)}.
\]
When $m$ and $n$ are of the same order, $I$ grows linearly with the problem
size (roughly $I \sim \tfrac{c}{8} \min(m,n)$).  For the largest configuration
we consider ($n_{\text{train}} = 32\text{k}$, $n_{\text{test}} = 4\text{k}$),
this yields
\[
  I
  \approx
  \frac{c\, (32\text{k}) (4\text{k})}{4\,(36\text{k})}
  \approx
  \frac{c}{4} \times 3.6 \times 10^3
  \;\gg\; 1 \quad \text{flop/byte}.
\]
Even with this approximate $c \approx 50$, the resulting intensity is orders
of magnitude larger than the machine-balance point of modern GPUs, so in
principle the kernel is compute-bound rather than bandwidth-bound.

\section{Empirical results}

We sweep $n_{\text{train}}$ over powers of two from $512$ to $32\text{k}$,
with $n_{\text{test}} = n_{\text{train}} / 8$, and average over three seeds.
For each configuration we record:
\begin{itemize}
  \item CPU Silverman KDE time,
  \item scikit-learn Gaussian KDE time, and
  \item empirical SD-KDE GPU time.
\end{itemize}
Across the entire range, the empirical SD-KDE GPU implementation is
consistently faster than scikit-learn, with speedups growing with $n$; at
the largest sizes the GPU achieves well over an order-of-magnitude speedup
relative to the sklearn baseline.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{../flash-sd-kde.pdf}
  \caption{Average runtime (log-scale $y$-axis) of scikit-learn KDE and
           empirical SD-KDE GPU across $n_{\text{train}}$; annotations show
           the empirical SD-KDE GPU speedup relative to sklearn.}
  \label{fig:flash_sd_kde}
\end{figure}

\section{GPU utilization on A6000}

For $n_{\text{train}} = 32\text{k}$ and $n_{\text{test}} = 4\text{k}$, we
observe an average empirical SD-KDE GPU runtime of approximately
$4\,\mathrm{ms}$.  There are
\[
  n_{\text{train}} \times n_{\text{test}}
  = 32{,}768 \times 4{,}096
  \approx 1.34 \times 10^8
\]
pairwise interactions.  Using the estimate $c \approx 30$ flops per
interaction, the total work is
\[
  \text{FLOPs} \approx 50 \times 1.34 \times 10^8
  \approx 6.7 \times 10^9.
\]
The achieved throughput is therefore
\[
  \frac{6.7 \times 10^9}{4 \times 10^{-3}}
  \approx 1.7 \times 10^{12}
  \;\text{FLOP/s} \;\approx\; 1.7\,\text{TFLOP/s}.
\]
The RTX~A6000 has a peak single-precision throughput of about
$40\,\text{TFLOP/s}$, so the achieved utilization is on the order of
\[
  \frac{1.7}{40} \approx 4\% .
\]
This modest utilization is expected for a first, straightforward Triton
implementation without extensive fusion or kernel tuning; nonetheless, the
kernel already delivers substantial end-to-end speedups over the CPU and
sklearn baselines while leaving significant headroom for further optimization.

\end{document}
